{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 10:34:27 WARN Utils: Your hostname, codespaces-b3fb74 resolves to a loopback address: 127.0.0.1; using 10.0.2.41 instead (on interface eth0)\n",
      "24/08/10 10:34:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/10 10:34:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/10 10:34:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession object\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate the cluster\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 15001 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|\n",
      "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| NULL|\n",
      "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|\n",
      "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
     ]
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "df = spark.read.csv('flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % df.count())\n",
    "\n",
    "# View the first five records\n",
    "display(df.show(5))\n",
    "\n",
    "# Check column data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mon: float (nullable = true)\n",
      " |-- dom: integer (nullable = true)\n",
      " |-- dow: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- depart: integer (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+-------+------+------+\n",
      "| mon|dom|dow|carrier|depart|flight|\n",
      "+----+---+---+-------+------+------+\n",
      "|10.0| 10|  1|     OO|  5836|   ORD|\n",
      "| 1.0|  4|  1|     OO|  5866|   ORD|\n",
      "|11.0| 22|  1|     OO|  6016|   ORD|\n",
      "| 2.0| 14|  5|     B6|   199|   JFK|\n",
      "| 5.0| 25|  3|     WN|  1675|   SJC|\n",
      "+----+---+---+-------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/06 06:20:56 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 10, schema size: 6\n",
      "CSV file: file:///workspaces/data_science_materials/pyspark/flights.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"mon\", FloatType()),\n",
    "    StructField(\"dom\", IntegerType()),\n",
    "    StructField(\"dow\", IntegerType()),\n",
    "    StructField(\"carrier\", StringType()),\n",
    "    StructField(\"depart\", IntegerType()),\n",
    "    StructField(\"flight\", StringType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "df = spark.read.csv('flights.csv', sep=',', header=True, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "display(df.printSchema())\n",
    "\n",
    "# View the first five records\n",
    "display(df.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Column & Remove Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15001\n",
      "915\n",
      "14086\n",
      "14086\n"
     ]
    }
   ],
   "source": [
    "# Remove the 'flight' column\n",
    "df_drop_column = df.drop('flight')\n",
    "print(df_drop_column.count())\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "print(df_drop_column.filter('delay IS NULL').count())\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "df_valid_delay = df_drop_column.filter('delay IS NOT NULL')\n",
    "print(df_valid_delay.count())\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "df_none_missing = df_valid_delay.dropna()\n",
    "print(df_none_missing.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "| 10| 10|  1|     OO|ORD|  8.18|      51|   27| 253.0|    1|\n",
      "| 11| 22|  1|     OO|ORD|  7.17|     127|  -19|1188.0|    0|\n",
      "|  2| 14|  5|     B6|JFK| 21.17|     365|   60|3618.0|    1|\n",
      "|  5| 25|  3|     WN|SJC| 12.92|      85|   22| 621.0|    1|\n",
      "|  3| 28|  1|     B6|LGA| 13.33|     182|   70|1732.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 10:34:38 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# Import the required function\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column (1 mile is equivalent to 1.60934 km)\n",
    "df_index = df_none_missing.withColumn('km', round(df.mile * 1.60934, 0)) \\\n",
    "                    .drop('mile')\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "df_index = df_index.withColumn('label', (df_index.delay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "df_index.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical to Numerical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "| 10| 10|  1|     OO|ORD|  8.18|      51|   27| 253.0|    1|        2.0|    0.0|\n",
      "| 11| 22|  1|     OO|ORD|  7.17|     127|  -19|1188.0|    0|        2.0|    0.0|\n",
      "|  2| 14|  5|     B6|JFK| 21.17|     365|   60|3618.0|    1|        4.0|    2.0|\n",
      "|  5| 25|  3|     WN|SJC| 12.92|      85|   22| 621.0|    1|        3.0|    4.0|\n",
      "|  3| 28|  1|     B6|LGA| 13.33|     182|   70|1732.0|    1|        4.0|    3.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(df_index)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "df_indexed = indexer_model.transform(df_index)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "df_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(df_indexed).transform(df_indexed)\n",
    "df_indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting train and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+-----+\n",
      "|features                                       |delay|\n",
      "+-----------------------------------------------+-----+\n",
      "|[10.0,10.0,1.0,2.0,0.0,253.0,8.18,51.0,27.0]   |27   |\n",
      "|[11.0,22.0,1.0,2.0,0.0,1188.0,7.17,127.0,-19.0]|-19  |\n",
      "|[2.0,14.0,5.0,4.0,2.0,3618.0,21.17,365.0,60.0] |60   |\n",
      "|[5.0,25.0,3.0,3.0,4.0,621.0,12.92,85.0,22.0]   |22   |\n",
      "|[3.0,28.0,1.0,4.0,3.0,1732.0,13.33,182.0,70.0] |70   |\n",
      "+-----------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'km', 'depart', 'duration', 'delay'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "df_assembled = assembler.transform(df_indexed)\n",
    "\n",
    "# Check the resulting column\n",
    "# df_assembled = df_assembled.select('features', 'delay')\n",
    "df_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9920 4166\n",
      "0.7042453499929008\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+---------------------------------------------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|km    |label|carrier_idx|org_idx|features                                     |\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+---------------------------------------------+\n",
      "|0  |1  |2  |AA     |ORD|8.42  |155     |83   |1617.0|1    |1.0        |0.0    |[0.0,1.0,2.0,1.0,0.0,1617.0,8.42,155.0,83.0] |\n",
      "|0  |1  |2  |AA     |ORD|15.25 |115     |20   |941.0 |1    |1.0        |0.0    |[0.0,1.0,2.0,1.0,0.0,941.0,15.25,115.0,20.0] |\n",
      "|0  |1  |2  |AA     |ORD|15.5  |90      |25   |649.0 |1    |1.0        |0.0    |[0.0,1.0,2.0,1.0,0.0,649.0,15.5,90.0,25.0]   |\n",
      "|0  |1  |2  |AA     |ORD|16.0  |135     |54   |1395.0|1    |1.0        |0.0    |[0.0,1.0,2.0,1.0,0.0,1395.0,16.0,135.0,54.0] |\n",
      "|0  |1  |2  |AA     |SJC|15.58 |205     |-3   |2314.0|0    |1.0        |4.0    |[0.0,1.0,2.0,1.0,4.0,2314.0,15.58,205.0,-3.0]|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "df_train, df_test = df_assembled.randomSplit([0.7, 0.3], seed=43)\n",
    "print(df_train.count(), df_test.count())\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = df_train.count() / df_assembled.count()\n",
    "print(training_ratio)\n",
    "df_train.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6924"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_assembled.select('label').filter(df_assembled.label==0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+\n",
      "|label|prediction|probability|\n",
      "+-----+----------+-----------+\n",
      "|0    |0.0       |[1.0,0.0]  |\n",
      "|1    |1.0       |[0.0,1.0]  |\n",
      "|1    |1.0       |[0.0,1.0]  |\n",
      "|1    |1.0       |[0.0,1.0]  |\n",
      "|1    |1.0       |[0.0,1.0]  |\n",
      "+-----+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Decision Tree Classifier class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(df_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(df_test)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|   19|\n",
      "|    0|       0.0| 2048|\n",
      "|    1|       1.0| 2095|\n",
      "|    0|       1.0|    4|\n",
      "+-----+----------+-----+\n",
      "\n",
      "0.9944791166586654\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label = 1').count()\n",
    "FP = prediction.filter('prediction = 1 AND label = 0').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 10:43:17 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/10 10:43:17 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0| 2052|\n",
      "|    1|       1.0| 2114|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(df_train)\n",
    "\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(df_test)\n",
    "\n",
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label = 1').count()\n",
    "FP = prediction.filter('prediction = 1 AND label = 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 1.00\n",
      "recall    = 1.00\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning Text into Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning & vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|text                                                                                                                                                       |v1  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                            |ham |\n",
      "|Ok lar... Joking wif u oni...                                                                                                                              |ham |\n",
      "|Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's|spam|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms = spark.read.csv('spam.csv', header=True).select(['v2', 'v1']).withColumnRenamed('v2', 'text').dropna()\n",
    "sms.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|text                                                                                                                                                       |v1  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|Go until jurong point  crazy   Available only in bugis n great world la e buffet    Cine there got amore wat                                               |ham |\n",
      "|Ok lar    Joking wif u oni                                                                                                                                 |ham |\n",
      "|Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005  Text FA to 87121 to receive entry question std txt rate T&C's apply 08452810075over18's|spam|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "|text                                                                                                                                                       |v1  |label|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "|Go until jurong point  crazy   Available only in bugis n great world la e buffet    Cine there got amore wat                                               |ham |1    |\n",
      "|Ok lar    Joking wif u oni                                                                                                                                 |ham |1    |\n",
      "|Free entry in   a wkly comp to win FA Cup final tkts   st May       Text FA to       to receive entry question std txt rate T&C's apply            over  's|spam|0    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "|text                                                                                                                           |v1  |label|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "|Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                         |ham |1    |\n",
      "|Ok lar Joking wif u oni                                                                                                        |ham |1    |\n",
      "|Free entry in a wkly comp to win FA Cup final tkts st May Text FA to to receive entry question std txt rate T&C's apply over 's|spam|0    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                           |v1  |label|words                                                                                                                                                       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                         |ham |1    |[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                 |\n",
      "|Ok lar Joking wif u oni                                                                                                        |ham |1    |[ok, lar, joking, wif, u, oni]                                                                                                                              |\n",
      "|Free entry in a wkly comp to win FA Cup final tkts st May Text FA to to receive entry question std txt rate T&C's apply over 's|spam|0    |[free, entry, in, a, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive, entry, question, std, txt, rate, t&c's, apply, over, 's]|\n",
      "|U dun say so early hor U c already then say                                                                                    |ham |1    |[u, dun, say, so, early, hor, u, c, already, then, say]                                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Remove punctuation (REGEX provided) and numbers\n",
    "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "display(wrangled.show(3, truncate=False))\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
    "wrangled = wrangled.withColumn('v1', regexp_replace(wrangled.v1, '\"\"\"', ''))\n",
    "wrangled = wrangled.withColumn(\"label\", F.when(wrangled.v1 == \"spam\", 0).otherwise(1))\n",
    "display(wrangled.show(3, truncate=False))\n",
    "\n",
    "# Merge multiple spaces\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
    "display(wrangled.show(3, truncate=False))\n",
    "\n",
    "# Split the text into words\n",
    "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
    "\n",
    "wrangled.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                           |v1  |label|words                                                                                                                                                       |terms                                                                                                                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                         |ham |1    |[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                 |[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                |\n",
      "|Ok lar Joking wif u oni                                                                                                        |ham |1    |[ok, lar, joking, wif, u, oni]                                                                                                                              |[ok, lar, joking, wif, u, oni]                                                                                                     |\n",
      "|Free entry in a wkly comp to win FA Cup final tkts st May Text FA to to receive entry question std txt rate T&C's apply over 's|spam|0    |[free, entry, in, a, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive, entry, question, std, txt, rate, t&c's, apply, over, 's]|[free, entry, wkly, comp, win, fa, cup, final, tkts, st, may, text, fa, receive, entry, question, std, txt, rate, t&c's, apply, 's]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                           |v1  |label|words                                                                                                                                                       |terms                                                                                                                              |hash                                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+\n",
      "|Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                         |ham |1    |[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                 |[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                |(24,[2,3,4,6,7,8,10,11,16,19,20,22],[1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0])|\n",
      "|Ok lar Joking wif u oni                                                                                                        |ham |1    |[ok, lar, joking, wif, u, oni]                                                                                                                              |[ok, lar, joking, wif, u, oni]                                                                                                     |(24,[6,13,15,19,21],[1.0,1.0,1.0,1.0,2.0])                                            |\n",
      "|Free entry in a wkly comp to win FA Cup final tkts st May Text FA to to receive entry question std txt rate T&C's apply over 's|spam|0    |[free, entry, in, a, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive, entry, question, std, txt, rate, t&c's, apply, over, 's]|[free, entry, wkly, comp, win, fa, cup, final, tkts, st, may, text, fa, receive, entry, question, std, txt, rate, t&c's, apply, 's]|(24,[0,1,4,5,6,9,16,17,18,19,21,22],[1.0,1.0,1.0,3.0,2.0,1.0,3.0,2.0,2.0,3.0,2.0,1.0])|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                           |v1  |label|words                                                                                                                                                       |terms                                                                                                                              |hash                                                                                  |features                                                                                                                                                                                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                         |ham |1    |[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                 |[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                |(24,[2,3,4,6,7,8,10,11,16,19,20,22],[1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0])|(24,[2,3,4,6,7,8,10,11,16,19,20,22],[0.9804705086132599,3.2525553244793475,1.4172678700816947,1.2556375662348125,1.3809266617048865,1.2040924143972467,1.5021954228372862,1.4113637938907497,4.2942940708246775,1.3895288652875497,1.380213140447358,1.4735993520093542])|\n",
      "|Ok lar Joking wif u oni                                                                                                        |ham |1    |[ok, lar, joking, wif, u, oni]                                                                                                                              |[ok, lar, joking, wif, u, oni]                                                                                                     |(24,[6,13,15,19,21],[1.0,1.0,1.0,1.0,2.0])                                            |(24,[6,13,15,19,21],[1.2556375662348125,1.1364561245331743,1.0482343754391654,1.3895288652875497,2.654846213067493])                                                                                                                                                     |\n",
      "|Free entry in a wkly comp to win FA Cup final tkts st May Text FA to to receive entry question std txt rate T&C's apply over 's|spam|0    |[free, entry, in, a, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive, entry, question, std, txt, rate, t&c's, apply, over, 's]|[free, entry, wkly, comp, win, fa, cup, final, tkts, st, may, text, fa, receive, entry, question, std, txt, rate, t&c's, apply, 's]|(24,[0,1,4,5,6,9,16,17,18,19,21,22],[1.0,1.0,1.0,3.0,2.0,1.0,3.0,2.0,2.0,3.0,2.0,1.0])|(24,[0,1,4,5,6,9,16,17,18,19,21,22],[1.131437993010328,1.1083473574388623,1.4172678700816947,3.414403339816347,2.511275132469625,1.2575285217305125,4.2942940708246775,2.786270143728557,1.995674162223117,4.168586595862649,2.654846213067493,1.4735993520093542])      |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Remove stop words.\n",
    "wrangled1 = StopWordsRemover(inputCol='words', outputCol='terms')\\\n",
    "      .transform(wrangled)\n",
    "display(wrangled1.show(3, truncate=False))\n",
    "\n",
    "# Apply the hashing trick\n",
    "wrangled2 = HashingTF(inputCol='terms', outputCol='hash', numFeatures=24)\\\n",
    "      .transform(wrangled1)\n",
    "display(wrangled2.show(3, truncate=False))\n",
    "\n",
    "# Convert hashed symbols to TF-IDF\n",
    "tf_idf = IDF(inputCol='hash', outputCol='features')\\\n",
    "      .fit(wrangled2).transform(wrangled2)\n",
    "# tf_idf = tf_idf.withColumnRenamed('v1','label')\n",
    "display(tf_idf.show(3, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    1|       0.0|   14|\n",
      "|    0|       0.0|   23|\n",
      "|    1|       1.0|  929|\n",
      "|    0|       1.0|  130|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "sms_train, sms_test = tf_idf.randomSplit([0.4, 0.1], seed=13)\n",
    "\n",
    "# Fit a Logistic Regression model to the training data\n",
    "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "prediction = logistic.transform(sms_test)\n",
    "\n",
    "# Create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n",
      "|org|org_idx|    org_dummy|\n",
      "+---+-------+-------------+\n",
      "|ORD|    0.0|(7,[0],[1.0])|\n",
      "|SFO|    1.0|(7,[1],[1.0])|\n",
      "|JFK|    2.0|(7,[2],[1.0])|\n",
      "|LGA|    3.0|(7,[3],[1.0])|\n",
      "|SJC|    4.0|(7,[4],[1.0])|\n",
      "|SMF|    5.0|(7,[5],[1.0])|\n",
      "|TUS|    6.0|(7,[6],[1.0])|\n",
      "|OGG|    7.0|    (7,[],[])|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoder(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(df_assembled)\n",
    "df_onehot = onehot.transform(df_assembled)\n",
    "\n",
    "# Check the results\n",
    "df_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression model w/o one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 10:44:16 WARN Instrumentation: [8602d0ad] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/10 10:44:17 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|duration|prediction        |\n",
      "+--------+------------------+\n",
      "|195     |194.9999999999996 |\n",
      "|120     |120.00000000000004|\n",
      "|265     |264.99999999999994|\n",
      "|65      |65.00000000000027 |\n",
      "|88      |87.99999999999986 |\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.1285340969800204e-13"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(df_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(df_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.564268359017664e-13\n",
      "[2.7417807914068246e-15,-6.667344699708082e-16,-2.092798120185854e-15,4.4310493738516057e-14,-3.70727804254414e-14,1.4244184878175099e-15,1.533773746157506e-15,0.9999999999999813,4.016862682672356e-16]\n",
      "2.7417807914068246e-15\n",
      "2.1883587553041988e+16\n"
     ]
    }
   ],
   "source": [
    "# Intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# Average minutes per km\n",
    "minutes_per_km = regression.coefficients[0]\n",
    "print(minutes_per_km)\n",
    "\n",
    "# Average speed in km per hour\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression model with one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 10:44:30 WARN Instrumentation: [714ff69b] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.3211762982106225e-13"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "df_onehot_train, df_onehot_test = df_onehot.randomSplit([0.8, 0.2], seed=13)\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(df_onehot_train)\n",
    "\n",
    "# Create predictions for the testing data\n",
    "predictions = regression.transform(df_onehot_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5803707886850709.0\n",
      "-1.21874977156918e-12\n",
      "-1.2539480943622785e-12\n",
      "-1.172498320259883e-12\n"
     ]
    }
   ],
   "source": [
    "# Average speed in km per hour\n",
    "avg_speed_hour = 60/regression.coefficients[0]\n",
    "print(avg_speed_hour)\n",
    "\n",
    "# Average minutes on ground at OGG\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Average minutes on ground at JFK\n",
    "avg_ground_jfk = inter + regression.coefficients[3]\n",
    "print(avg_ground_jfk)\n",
    "\n",
    "# Average minutes on ground at LGA\n",
    "avg_ground_lga = inter + regression.coefficients[4]\n",
    "print(avg_ground_lga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing & One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|depart|depart_bucket|\n",
      "+------+-------------+\n",
      "|  8.18|          2.0|\n",
      "|  7.17|          2.0|\n",
      "| 21.17|          7.0|\n",
      "| 12.92|          4.0|\n",
      "| 13.33|          4.0|\n",
      "+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------+\n",
      "|depart|depart_bucket| depart_dummy|\n",
      "+------+-------------+-------------+\n",
      "|  8.18|          2.0|(7,[2],[1.0])|\n",
      "|  7.17|          2.0|(7,[2],[1.0])|\n",
      "| 21.17|          7.0|    (7,[],[])|\n",
      "| 12.92|          4.0|(7,[4],[1.0])|\n",
      "| 13.33|          4.0|(7,[4],[1.0])|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoder\n",
    "\n",
    "# Create buckets at 3 hour intervals through the day\n",
    "buckets = Bucketizer(splits=[0,3,6,9,12,15,18,21,24], inputCol='depart', outputCol='depart_bucket')\n",
    "\n",
    "# Bucket the departure times\n",
    "bucketed = buckets.transform(df_onehot)\n",
    "display(bucketed.select('depart','depart_bucket').show(5))\n",
    "\n",
    "# Create a one-hot encoder\n",
    "onehot = OneHotEncoder(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
    "\n",
    "# One-hot encode the bucketed departure times\n",
    "df_onehot1 = onehot.fit(bucketed).transform(bucketed)\n",
    "df_onehot1.select('depart', 'depart_bucket', 'depart_dummy').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso & Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 1.0214000829509382\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9885520847824293,0.0]\n",
      "Number of coefficients equal to 0: 8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Fit Lasso model (λ = 1, α = 1) to training data\n",
    "regression = LinearRegression(labelCol='duration', regParam=1, elasticNetParam=1).fit(df_train)\n",
    "# elasticNetParam: {0 for ridge & 1 for lasso}\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol='duration').evaluate(regression.transform(df_test))\n",
    "print(\"The test RMSE is\", rmse)\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)\n",
    "# Number of zero coefficients\n",
    "zero_coeff = sum([beta==0 for beta in regression.coefficients])\n",
    "print(\"Number of coefficients equal to 0:\", zero_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical strings to index values\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "\n",
    "# One-hot encode index values\n",
    "onehot = OneHotEncoder(\n",
    "    inputCols=['org_idx', 'dow'],\n",
    "    outputCols=['org_dummy', 'dow_dummy']\n",
    ")\n",
    "\n",
    "# Assemble predictors into a single column\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy', 'dow_dummy'], outputCol='features')\n",
    "\n",
    "# A linear regression object\n",
    "regression = LinearRegression(labelCol='duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mon',\n",
       " 'dom',\n",
       " 'dow',\n",
       " 'carrier',\n",
       " 'org',\n",
       " 'depart',\n",
       " 'duration',\n",
       " 'delay',\n",
       " 'km',\n",
       " 'label']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_index.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 10:51:06 WARN Instrumentation: [316883d8] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "# Import class for creating a pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Construct a pipeline\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "df_train, df_test = df_index.randomSplit([0.8, 0.2], seed=43)\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "pipeline = pipeline.fit(df_train)\n",
    "\n",
    "# # Make predictions on the testing data\n",
    "# predictions = pipeline.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Break text into tokens at non-word characters\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='terms')\n",
    "\n",
    "# Apply the hashing trick and transform to TF-IDF\n",
    "hasher = HashingTF(inputCol=\"terms\", outputCol=\"hash\")\n",
    "idf = IDF(inputCol=\"hash\", outputCol=\"features\")\n",
    "\n",
    "# Create a logistic regression object and add everything to a pipeline\n",
    "logistic = LogisticRegression()\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an assembler object\n",
    "# assembler = VectorAssembler(inputCols=[\n",
    "#     'mon', 'depart', 'duration'\n",
    "# ], outputCol='features')\n",
    "\n",
    "# # Consolidate predictor columns\n",
    "# df_index_assembler = assembler.transform(df_index.select('mon', 'dow', 'org', 'depart', 'duration', 'km', 'label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+-----+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|   km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+-----+-----+-----------+-------+\n",
      "| 10| 10|  1|     OO|ORD|  8.18|      51|   27|253.0|    1|        2.0|    0.0|\n",
      "+---+---+---+-------+---+------+--------+-----+-----+-----+-----------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_indexed.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 10:53:23 WARN Instrumentation: [a35e6320] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/10 10:53:24 WARN Instrumentation: [de4ae38f] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/10 10:53:25 WARN Instrumentation: [29a8335d] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/10 10:53:26 WARN Instrumentation: [ad19039f] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/10 10:53:26 WARN Instrumentation: [c589f20e] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/10 10:53:27 WARN Instrumentation: [b13d3a94] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create an empty parameter grid\n",
    "params = ParamGridBuilder().build()\n",
    "\n",
    "# Create objects for building and evaluating a regression model\n",
    "regression = LinearRegression(labelCol='duration')\n",
    "evaluator = RegressionEvaluator(labelCol='duration')\n",
    "\n",
    "# Create a cross validator\n",
    "cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "df_assembler = assembler.transform(df_indexed)\n",
    "\n",
    "df_train, df_test = df_assembler.randomSplit([0.8, 0.2], seed=43)\n",
    "# Train and test model on multiple folds of the training data\n",
    "cv = cv.fit(df_train)\n",
    "\n",
    "# NOTE: Since cross-valdiation builds multiple models, the fit() method can take a little while to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer for the org field\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "\n",
    "# Create an one-hot encoder for the indexed org field\n",
    "onehot = OneHotEncoder(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Assemble the km and one-hot encoded fields\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy'], outputCol='features')\n",
    "\n",
    "# Create a pipeline and cross-validator.\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=params,\n",
    "                    evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to be tested:  12\n"
     ]
    }
   ],
   "source": [
    "# Create parameter grid\n",
    "params = ParamGridBuilder()\n",
    "\n",
    "# Add grids for two parameters\n",
    "params = params.addGrid(regression.regParam, [0.01, 0.1, 1.0, 10.0]) \\\n",
    "               .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "\n",
    "# Build the parameter grid\n",
    "params = params.build()\n",
    "print('Number of models to be tested: ', len(params))\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/10 11:06:36 WARN CacheManager: Asked to cache already cached data.\n",
      "24/08/10 11:06:36 WARN CacheManager: Asked to cache already cached data.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CrossValidator' object has no attribute 'bestModel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m cv\u001b[38;5;241m.\u001b[39mfit(df_train, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Get the best model from cross validation\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbestModel\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Look at the stages in the best model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_model\u001b[38;5;241m.\u001b[39mstages)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CrossValidator' object has no attribute 'bestModel'"
     ]
    }
   ],
   "source": [
    "df_train, df_test = df_index.randomSplit([0.8, 0.2], seed=43)\n",
    "\n",
    "# Model Fit\n",
    "cv.fit(df_train, params=params)\n",
    "\n",
    "# Get the best model from cross validation\n",
    "best_model = cv.bestModel\n",
    "\n",
    "# Look at the stages in the best model\n",
    "print(best_model.stages)\n",
    "\n",
    "# Get the parameters for the LinearRegression object in the best model\n",
    "best_model.stages[3].extractParamMap()\n",
    "\n",
    "# Generate predictions on testing data using the best model then calculate RMSE\n",
    "predictions = best_model.transform(df_test)\n",
    "print(\"RMSE =\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-----+----------------+\n",
      "|mon|depart|duration|label|        features|\n",
      "+---+------+--------+-----+----------------+\n",
      "|  0|  5.75|      50|    0| [0.0,5.75,50.0]|\n",
      "|  0|  5.75|     119|    0|[0.0,5.75,119.0]|\n",
      "+---+------+--------+-----+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'depart', 'duration'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "df_index_assembler = assembler.transform(df_index.select('mon', 'depart', 'duration', 'label'))\n",
    "\n",
    "df_train, df_test = df_index_assembler.randomSplit([0.8, 0.2], seed=43)\n",
    "df_train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6401995200516241\n",
      "0.6807341881901746\n",
      "[DecisionTreeRegressionModel: uid=dtr_0c400ce796bd, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_dea0e8edbb12, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_7b211d22856c, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_bd5c564eecac, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_7f6caa740f5c, depth=5, numNodes=61, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_514fe69367c6, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_2f285047fe9d, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_96fa2f30cef9, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_f26a192786e7, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_fdb7958fa537, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_0f6944fc0503, depth=5, numNodes=61, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_6328e00d8eb4, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_14ed167a19d3, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_e4212943efb2, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_cffe0e6b719d, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_2a9748b7de7f, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_fbc266766c2c, depth=5, numNodes=61, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_a11893543508, depth=5, numNodes=61, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_729495ad4223, depth=5, numNodes=63, numFeatures=3, DecisionTreeRegressionModel: uid=dtr_bb2f28f1586b, depth=5, numNodes=63, numFeatures=3]\n",
      "(3,[0,1,2],[0.2698895026432686,0.3387105457213685,0.3913999516353628])\n"
     ]
    }
   ],
   "source": [
    "# Import the classes required\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# Create model objects and train on training data\n",
    "tree = DecisionTreeClassifier().fit(df_train)\n",
    "gbt = GBTClassifier().fit(df_train)\n",
    "# Compare AUC on testing data\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "print(evaluator.evaluate(tree.transform(df_test)))\n",
    "print(evaluator.evaluate(gbt.transform(df_test)))\n",
    "# Find the number of trees and the relative importance of features\n",
    "print(gbt.trees)\n",
    "print(gbt.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "# Create a random forest classifier\n",
    "forest = RandomForestClassifier()\n",
    "# Create a parameter grid\n",
    "params = ParamGridBuilder() \\\n",
    "            .addGrid(forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']) \\\n",
    "            .addGrid(forest.maxDepth, [2, 5, 10]) \\\n",
    "            .build()\n",
    "# Create a binary classification evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "# Create a cross-validator\n",
    "cv = CrossValidator(estimator=forest, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "forest = RandomForestClassifier()\n",
    "# Create a parameter grid\n",
    "params = ParamGridBuilder() \\\n",
    "            .addGrid(forest.featureSubsetStrategy, ['all', 'onethird', 'sqrt', 'log2']) \\\n",
    "            .addGrid(forest.maxDepth, [2, 5, 10]) \\\n",
    "            .build()\n",
    "# Create a binary classification evaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "# Create a cross-validator\n",
    "cv = CrossValidator(estimator=forest, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CrossValidator' object has no attribute 'avgMetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Average AUC for each parameter combination in grid\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavgMetrics\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Average AUC for the best model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(cv\u001b[38;5;241m.\u001b[39mavgMetrics))  \u001b[38;5;66;03m# 0.68\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CrossValidator' object has no attribute 'avgMetrics'"
     ]
    }
   ],
   "source": [
    "# Average AUC for each parameter combination in grid\n",
    "print(cv.avgMetrics)\n",
    "# Average AUC for the best model\n",
    "print(max(cv.avgMetrics))  # 0.68\n",
    "# What's the optimal parameter value for maxDepth?\n",
    "print(cv.bestModel.explainParam('maxDepth'))\n",
    "# What's the optimal parameter value for featureSubsetStrategy?\n",
    "print(cv.bestModel.explainParam('featureSubsetStrategy'))\n",
    "# AUC for best model on testing data\n",
    "print(evaluator.evaluate(cv.bestModel.transform(flights_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
