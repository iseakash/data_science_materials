{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/08 06:20:08 WARN Utils: Your hostname, codespaces-648539 resolves to a loopback address: 127.0.0.1; using 10.0.0.70 instead (on interface eth0)\n",
      "24/08/08 06:20:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/08 06:20:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession object\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate the cluster\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data contain 15001 records.\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|\n",
      "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| NULL|\n",
      "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|\n",
      "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|\n",
      "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|\n",
      "+---+---+---+-------+------+---+----+------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
     ]
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "df = spark.read.csv('flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % df.count())\n",
    "\n",
    "# View the first five records\n",
    "display(df.show(5))\n",
    "\n",
    "# Check column data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mon: float (nullable = true)\n",
      " |-- dom: integer (nullable = true)\n",
      " |-- dow: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- depart: integer (nullable = true)\n",
      " |-- flight: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+-------+------+------+\n",
      "| mon|dom|dow|carrier|depart|flight|\n",
      "+----+---+---+-------+------+------+\n",
      "|10.0| 10|  1|     OO|  5836|   ORD|\n",
      "| 1.0|  4|  1|     OO|  5866|   ORD|\n",
      "|11.0| 22|  1|     OO|  6016|   ORD|\n",
      "| 2.0| 14|  5|     B6|   199|   JFK|\n",
      "| 5.0| 25|  3|     WN|  1675|   SJC|\n",
      "+----+---+---+-------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/06 06:20:56 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 10, schema size: 6\n",
      "CSV file: file:///workspaces/data_science_materials/pyspark/flights.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "# from pyspark.sql.types import *\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"mon\", FloatType()),\n",
    "    StructField(\"dom\", IntegerType()),\n",
    "    StructField(\"dow\", IntegerType()),\n",
    "    StructField(\"carrier\", StringType()),\n",
    "    StructField(\"depart\", IntegerType()),\n",
    "    StructField(\"flight\", StringType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "df = spark.read.csv('flights.csv', sep=',', header=True, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "display(df.printSchema())\n",
    "\n",
    "# View the first five records\n",
    "display(df.show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Column & Remove Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15001\n",
      "915\n",
      "14086\n",
      "14086\n"
     ]
    }
   ],
   "source": [
    "# Remove the 'flight' column\n",
    "df_drop_column = df.drop('flight')\n",
    "print(df_drop_column.count())\n",
    "\n",
    "# Number of records with missing 'delay' values\n",
    "print(df_drop_column.filter('delay IS NULL').count())\n",
    "\n",
    "# Remove records with missing 'delay' values\n",
    "df_valid_delay = df_drop_column.filter('delay IS NOT NULL')\n",
    "print(df_valid_delay.count())\n",
    "\n",
    "# Remove records with missing values in any column and get the number of remaining rows\n",
    "df_none_missing = df_valid_delay.dropna()\n",
    "print(df_none_missing.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "| 10| 10|  1|     OO|ORD|  8.18|      51|   27| 253.0|    1|\n",
      "| 11| 22|  1|     OO|ORD|  7.17|     127|  -19|1188.0|    0|\n",
      "|  2| 14|  5|     B6|JFK| 21.17|     365|   60|3618.0|    1|\n",
      "|  5| 25|  3|     WN|SJC| 12.92|      85|   22| 621.0|    1|\n",
      "|  3| 28|  1|     B6|LGA| 13.33|     182|   70|1732.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the required function\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# Convert 'mile' to 'km' and drop 'mile' column (1 mile is equivalent to 1.60934 km)\n",
    "df_indexed = df_none_missing.withColumn('km', round(df.mile * 1.60934, 0)) \\\n",
    "                    .drop('mile')\n",
    "\n",
    "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
    "df_indexed = df_indexed.withColumn('label', (df_indexed.delay >= 15).cast('integer'))\n",
    "\n",
    "# Check first five records\n",
    "df_indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical to Numerical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|carrier_idx|org_idx|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "| 10| 10|  1|     OO|ORD|  8.18|      51|   27| 253.0|    1|        2.0|    0.0|\n",
      "| 11| 22|  1|     OO|ORD|  7.17|     127|  -19|1188.0|    0|        2.0|    0.0|\n",
      "|  2| 14|  5|     B6|JFK| 21.17|     365|   60|3618.0|    1|        4.0|    2.0|\n",
      "|  5| 25|  3|     WN|SJC| 12.92|      85|   22| 621.0|    1|        3.0|    4.0|\n",
      "|  3| 28|  1|     B6|LGA| 13.33|     182|   70|1732.0|    1|        4.0|    3.0|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create an indexer\n",
    "indexer = StringIndexer(inputCol='carrier', outputCol='carrier_idx')\n",
    "\n",
    "# Indexer identifies categories in the data\n",
    "indexer_model = indexer.fit(df_indexed)\n",
    "\n",
    "# Indexer creates a new column with numeric index values\n",
    "df_indexed = indexer_model.transform(df_indexed)\n",
    "\n",
    "# Repeat the process for the other categorical feature\n",
    "df_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(df_indexed).transform(df_indexed)\n",
    "df_indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting train and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+-----+\n",
      "|features                                           |delay|\n",
      "+---------------------------------------------------+-----+\n",
      "|[10.0,10.0,1.0,2.0,0.0,253.0,8.18,51.0,1.0,27.0]   |27   |\n",
      "|[11.0,22.0,1.0,2.0,0.0,1188.0,7.17,127.0,0.0,-19.0]|-19  |\n",
      "|[2.0,14.0,5.0,4.0,2.0,3618.0,21.17,365.0,1.0,60.0] |60   |\n",
      "|[5.0,25.0,3.0,3.0,4.0,621.0,12.92,85.0,1.0,22.0]   |22   |\n",
      "|[3.0,28.0,1.0,4.0,3.0,1732.0,13.33,182.0,1.0,70.0] |70   |\n",
      "+---------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary class\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    'mon', 'dom', 'dow', 'carrier_idx', 'org_idx', 'km', 'depart', 'duration', 'label', 'delay'\n",
    "], outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "df_assembled = assembler.transform(df_indexed)\n",
    "\n",
    "# Check the resulting column\n",
    "# df_assembled = df_assembled.select('features', 'delay')\n",
    "df_assembled.select('features', 'delay').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11320 2766\n",
      "0.8036348147096408\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+------------------------------------------------+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|km    |label|carrier_idx|org_idx|features                                        |\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+------------------------------------------------+\n",
      "|0  |1  |2  |AA     |ORD|8.42  |155     |83   |1617.0|1    |1.0        |0.0    |[0.0,1.0,2.0,1.0,0.0,1617.0,8.42,155.0,1.0,83.0]|\n",
      "|0  |1  |2  |AA     |ORD|15.25 |115     |20   |941.0 |1    |1.0        |0.0    |[0.0,1.0,2.0,1.0,0.0,941.0,15.25,115.0,1.0,20.0]|\n",
      "|0  |1  |2  |AA     |ORD|15.5  |90      |25   |649.0 |1    |1.0        |0.0    |[0.0,1.0,2.0,1.0,0.0,649.0,15.5,90.0,1.0,25.0]  |\n",
      "|0  |1  |2  |AA     |ORD|16.0  |135     |54   |1395.0|1    |1.0        |0.0    |[0.0,1.0,2.0,1.0,0.0,1395.0,16.0,135.0,1.0,54.0]|\n",
      "|0  |1  |2  |AA     |ORD|21.5  |65      |133  |415.0 |1    |1.0        |0.0    |[0.0,1.0,2.0,1.0,0.0,415.0,21.5,65.0,1.0,133.0] |\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+-----------+-------+------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets in a 80:20 ratio\n",
    "df_train, df_test = df_assembled.randomSplit([0.8, 0.2], seed=43)\n",
    "print(df_train.count(), df_test.count())\n",
    "\n",
    "# Check that training set has around 80% of records\n",
    "training_ratio = df_train.count() / df_assembled.count()\n",
    "print(training_ratio)\n",
    "df_train.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+\n",
      "|label|prediction|probability|\n",
      "+-----+----------+-----------+\n",
      "|1    |1.0       |[0.0,1.0]  |\n",
      "|0    |0.0       |[1.0,0.0]  |\n",
      "|1    |1.0       |[0.0,1.0]  |\n",
      "|1    |1.0       |[0.0,1.0]  |\n",
      "|1    |1.0       |[0.0,1.0]  |\n",
      "+-----+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Decision Tree Classifier class\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier()\n",
    "tree_model = tree.fit(df_assembled)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(df_assembled)\n",
    "prediction.select('label', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0| 6924|\n",
      "|    1|       1.0| 7162|\n",
      "+-----+----------+-----+\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()\n",
    "\n",
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label = 1').count()\n",
    "FP = prediction.filter('prediction = 1 AND label = 0').count()\n",
    "\n",
    "# Accuracy measures the proportion of correct predictions\n",
    "accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/07 08:46:14 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/07 08:46:14 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|    0|       0.0| 1380|\n",
      "|    1|       1.0| 1386|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the logistic regression class\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create a classifier object and train on training data\n",
    "logistic = LogisticRegression().fit(df_train)\n",
    "\n",
    "# Create predictions for the testing data and show confusion matrix\n",
    "prediction = logistic.transform(df_test)\n",
    "\n",
    "# Create a confusion matrix\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND label = 1').count()\n",
    "FP = prediction.filter('prediction = 1 AND label = 0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 1.00\n",
      "recall    = 1.00\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "print('precision = {:.2f}\\nrecall    = {:.2f}'.format(precision, recall))\n",
    "\n",
    "# Find weighted precision\n",
    "multi_evaluator = MulticlassClassificationEvaluator()\n",
    "weighted_precision = multi_evaluator.evaluate(prediction, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Find AUC\n",
    "binary_evaluator = BinaryClassificationEvaluator()\n",
    "auc = binary_evaluator.evaluate(prediction, {binary_evaluator.metricName: \"areaUnderROC\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning Text into Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text cleaning & vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|text                                                                                                                                                       |v1  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                            |ham |\n",
      "|Ok lar... Joking wif u oni...                                                                                                                              |ham |\n",
      "|Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's|spam|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sms = spark.read.csv('spam.csv', header=True).select(['v2', 'v1']).withColumnRenamed('v2', 'text').dropna()\n",
    "sms.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|text                                                                                                                                                       |v1  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|Go until jurong point  crazy   Available only in bugis n great world la e buffet    Cine there got amore wat                                               |ham |\n",
      "|Ok lar    Joking wif u oni                                                                                                                                 |ham |\n",
      "|Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005  Text FA to 87121 to receive entry question std txt rate T&C's apply 08452810075over18's|spam|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "|text                                                                                                                                                       |v1  |label|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "|Go until jurong point  crazy   Available only in bugis n great world la e buffet    Cine there got amore wat                                               |ham |1    |\n",
      "|Ok lar    Joking wif u oni                                                                                                                                 |ham |1    |\n",
      "|Free entry in   a wkly comp to win FA Cup final tkts   st May       Text FA to       to receive entry question std txt rate T&C's apply            over  's|spam|0    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "|text                                                                                                                           |v1  |label|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "|Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                         |ham |1    |\n",
      "|Ok lar Joking wif u oni                                                                                                        |ham |1    |\n",
      "|Free entry in a wkly comp to win FA Cup final tkts st May Text FA to to receive entry question std txt rate T&C's apply over 's|spam|0    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                           |v1  |label|words                                                                                                                                                       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                         |ham |1    |[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                 |\n",
      "|Ok lar Joking wif u oni                                                                                                        |ham |1    |[ok, lar, joking, wif, u, oni]                                                                                                                              |\n",
      "|Free entry in a wkly comp to win FA Cup final tkts st May Text FA to to receive entry question std txt rate T&C's apply over 's|spam|0    |[free, entry, in, a, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive, entry, question, std, txt, rate, t&c's, apply, over, 's]|\n",
      "|U dun say so early hor U c already then say                                                                                    |ham |1    |[u, dun, say, so, early, hor, u, c, already, then, say]                                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Remove punctuation (REGEX provided) and numbers\n",
    "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
    "display(wrangled.show(3, truncate=False))\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, '[0-9]', ' '))\n",
    "wrangled = wrangled.withColumn('v1', regexp_replace(wrangled.v1, '\"\"\"', ''))\n",
    "wrangled = wrangled.withColumn(\"label\", F.when(wrangled.v1 == \"spam\", 0).otherwise(1))\n",
    "display(wrangled.show(3, truncate=False))\n",
    "\n",
    "# Merge multiple spaces\n",
    "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
    "display(wrangled.show(3, truncate=False))\n",
    "\n",
    "# Split the text into words\n",
    "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
    "\n",
    "wrangled.show(4, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                           |v1  |label|words                                                                                                                                                       |terms                                                                                                                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                         |ham |1    |[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                 |[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                |\n",
      "|Ok lar Joking wif u oni                                                                                                        |ham |1    |[ok, lar, joking, wif, u, oni]                                                                                                                              |[ok, lar, joking, wif, u, oni]                                                                                                     |\n",
      "|Free entry in a wkly comp to win FA Cup final tkts st May Text FA to to receive entry question std txt rate T&C's apply over 's|spam|0    |[free, entry, in, a, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive, entry, question, std, txt, rate, t&c's, apply, over, 's]|[free, entry, wkly, comp, win, fa, cup, final, tkts, st, may, text, fa, receive, entry, question, std, txt, rate, t&c's, apply, 's]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                           |v1  |label|words                                                                                                                                                       |terms                                                                                                                              |hash                                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+\n",
      "|Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                         |ham |1    |[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                 |[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                |(24,[2,3,4,6,7,8,10,11,16,19,20,22],[1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0])|\n",
      "|Ok lar Joking wif u oni                                                                                                        |ham |1    |[ok, lar, joking, wif, u, oni]                                                                                                                              |[ok, lar, joking, wif, u, oni]                                                                                                     |(24,[6,13,15,19,21],[1.0,1.0,1.0,1.0,2.0])                                            |\n",
      "|Free entry in a wkly comp to win FA Cup final tkts st May Text FA to to receive entry question std txt rate T&C's apply over 's|spam|0    |[free, entry, in, a, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive, entry, question, std, txt, rate, t&c's, apply, over, 's]|[free, entry, wkly, comp, win, fa, cup, final, tkts, st, may, text, fa, receive, entry, question, std, txt, rate, t&c's, apply, 's]|(24,[0,1,4,5,6,9,16,17,18,19,21,22],[1.0,1.0,1.0,3.0,2.0,1.0,3.0,2.0,2.0,3.0,2.0,1.0])|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                           |v1  |label|words                                                                                                                                                       |terms                                                                                                                              |hash                                                                                  |features                                                                                                                                                                                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Go until jurong point crazy Available only in bugis n great world la e buffet Cine there got amore wat                         |ham |1    |[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]                                 |[go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]                                |(24,[2,3,4,6,7,8,10,11,16,19,20,22],[1.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,3.0,1.0,1.0,1.0])|(24,[2,3,4,6,7,8,10,11,16,19,20,22],[0.9804705086132599,3.2525553244793475,1.4172678700816947,1.2556375662348125,1.3809266617048865,1.2040924143972467,1.5021954228372862,1.4113637938907497,4.2942940708246775,1.3895288652875497,1.380213140447358,1.4735993520093542])|\n",
      "|Ok lar Joking wif u oni                                                                                                        |ham |1    |[ok, lar, joking, wif, u, oni]                                                                                                                              |[ok, lar, joking, wif, u, oni]                                                                                                     |(24,[6,13,15,19,21],[1.0,1.0,1.0,1.0,2.0])                                            |(24,[6,13,15,19,21],[1.2556375662348125,1.1364561245331743,1.0482343754391654,1.3895288652875497,2.654846213067493])                                                                                                                                                     |\n",
      "|Free entry in a wkly comp to win FA Cup final tkts st May Text FA to to receive entry question std txt rate T&C's apply over 's|spam|0    |[free, entry, in, a, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive, entry, question, std, txt, rate, t&c's, apply, over, 's]|[free, entry, wkly, comp, win, fa, cup, final, tkts, st, may, text, fa, receive, entry, question, std, txt, rate, t&c's, apply, 's]|(24,[0,1,4,5,6,9,16,17,18,19,21,22],[1.0,1.0,1.0,3.0,2.0,1.0,3.0,2.0,2.0,3.0,2.0,1.0])|(24,[0,1,4,5,6,9,16,17,18,19,21,22],[1.131437993010328,1.1083473574388623,1.4172678700816947,3.414403339816347,2.511275132469625,1.2575285217305125,4.2942940708246775,2.786270143728557,1.995674162223117,4.168586595862649,2.654846213067493,1.4735993520093542])      |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------+----+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Remove stop words.\n",
    "wrangled1 = StopWordsRemover(inputCol='words', outputCol='terms')\\\n",
    "      .transform(wrangled)\n",
    "display(wrangled1.show(3, truncate=False))\n",
    "\n",
    "# Apply the hashing trick\n",
    "wrangled2 = HashingTF(inputCol='terms', outputCol='hash', numFeatures=24)\\\n",
    "      .transform(wrangled1)\n",
    "display(wrangled2.show(3, truncate=False))\n",
    "\n",
    "# Convert hashed symbols to TF-IDF\n",
    "tf_idf = IDF(inputCol='hash', outputCol='features')\\\n",
    "      .fit(wrangled2).transform(wrangled2)\n",
    "# tf_idf = tf_idf.withColumnRenamed('v1','label')\n",
    "display(tf_idf.show(3, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m sms_train, sms_test \u001b[38;5;241m=\u001b[39m tf_idf\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.1\u001b[39m], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Fit a Logistic Regression model to the training data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m logistic \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m(regParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(sms_train)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Make predictions on the testing data\u001b[39;00m\n\u001b[1;32m      8\u001b[0m prediction \u001b[38;5;241m=\u001b[39m logistic\u001b[38;5;241m.\u001b[39mtransform(sms_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "sms_train, sms_test = tf_idf.randomSplit([0.4, 0.1], seed=13)\n",
    "\n",
    "# Fit a Logistic Regression model to the training data\n",
    "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "prediction = logistic.transform(sms_test)\n",
    "\n",
    "# Create a confusion matrix, comparing predictions to known labels\n",
    "prediction.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+\n",
      "|org|org_idx|    org_dummy|\n",
      "+---+-------+-------------+\n",
      "|ORD|    0.0|(7,[0],[1.0])|\n",
      "|SFO|    1.0|(7,[1],[1.0])|\n",
      "|JFK|    2.0|(7,[2],[1.0])|\n",
      "|LGA|    3.0|(7,[3],[1.0])|\n",
      "|SJC|    4.0|(7,[4],[1.0])|\n",
      "|SMF|    5.0|(7,[5],[1.0])|\n",
      "|TUS|    6.0|(7,[6],[1.0])|\n",
      "|OGG|    7.0|    (7,[],[])|\n",
      "+---+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoder(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(df_assembled)\n",
    "df_onehot = onehot.transform(df_assembled)\n",
    "\n",
    "# Check the results\n",
    "df_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression model w/o one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/08 06:21:37 WARN Instrumentation: [258f8c3c] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/08 06:21:38 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/08 06:21:38 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "24/08/08 06:21:38 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|duration|prediction        |\n",
      "+--------+------------------+\n",
      "|195     |194.9999999999996 |\n",
      "|120     |120.00000000000004|\n",
      "|265     |265.0             |\n",
      "|88      |87.9999999999999  |\n",
      "|117     |116.99999999999932|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.8485105272259854e-13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(df_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(df_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.617008537181419e-13\n",
      "[-1.0804753471538299e-15,-1.2639879735261694e-15,-1.9506819523326384e-15,4.007560407165759e-14,-3.327950564692983e-14,1.2812678428908628e-15,1.1568848562433283e-15,0.9999999999999833,3.976028718197154e-14,1.3931928548436367e-16]\n",
      "-1.0804753471538299e-15\n",
      "-5.553111429895277e+16\n"
     ]
    }
   ],
   "source": [
    "# Intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# Average minutes per km\n",
    "minutes_per_km = regression.coefficients[0]\n",
    "print(minutes_per_km)\n",
    "\n",
    "# Average speed in km per hour\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression model with one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/08 06:21:40 WARN Instrumentation: [417ed216] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.329158461196226e-13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "df_onehot_train, df_onehot_test = df_onehot.randomSplit([0.8, 0.2], seed=13)\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(df_onehot_train)\n",
    "\n",
    "# Create predictions for the testing data\n",
    "predictions = regression.transform(df_onehot_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5934138335959200.0\n",
      "-1.216034840414051e-12\n",
      "-1.2518326973674596e-12\n",
      "-1.1691513757719602e-12\n"
     ]
    }
   ],
   "source": [
    "# Average speed in km per hour\n",
    "avg_speed_hour = 60/regression.coefficients[0]\n",
    "print(avg_speed_hour)\n",
    "\n",
    "# Average minutes on ground at OGG\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Average minutes on ground at JFK\n",
    "avg_ground_jfk = inter + regression.coefficients[3]\n",
    "print(avg_ground_jfk)\n",
    "\n",
    "# Average minutes on ground at LGA\n",
    "avg_ground_lga = inter + regression.coefficients[4]\n",
    "print(avg_ground_lga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing & One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|depart|depart_bucket|\n",
      "+------+-------------+\n",
      "|  8.18|          2.0|\n",
      "|  7.17|          2.0|\n",
      "| 21.17|          7.0|\n",
      "| 12.92|          4.0|\n",
      "| 13.33|          4.0|\n",
      "+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------+\n",
      "|depart|depart_bucket| depart_dummy|\n",
      "+------+-------------+-------------+\n",
      "|  8.18|          2.0|(7,[2],[1.0])|\n",
      "|  7.17|          2.0|(7,[2],[1.0])|\n",
      "| 21.17|          7.0|    (7,[],[])|\n",
      "| 12.92|          4.0|(7,[4],[1.0])|\n",
      "| 13.33|          4.0|(7,[4],[1.0])|\n",
      "+------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoder\n",
    "\n",
    "# Create buckets at 3 hour intervals through the day\n",
    "buckets = Bucketizer(splits=[0,3,6,9,12,15,18,21,24], inputCol='depart', outputCol='depart_bucket')\n",
    "\n",
    "# Bucket the departure times\n",
    "bucketed = buckets.transform(df_onehot)\n",
    "display(bucketed.select('depart','depart_bucket').show(5))\n",
    "\n",
    "# Create a one-hot encoder\n",
    "onehot = OneHotEncoder(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
    "\n",
    "# One-hot encode the bucketed departure times\n",
    "df_onehot1 = onehot.fit(bucketed).transform(bucketed)\n",
    "df_onehot1.select('depart', 'depart_bucket', 'depart_dummy').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso & Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test RMSE is 1.0074307566813816\n",
      "[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9886078566829308,0.0,0.0]\n",
      "Number of coefficients equal to 0: 9\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Fit Lasso model (λ = 1, α = 1) to training data\n",
    "regression = LinearRegression(labelCol='duration', regParam=1, elasticNetParam=1).fit(df_train)\n",
    "# elasticNetParam: {0 for ridge & 1 for lasso}\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol='duration').evaluate(regression.transform(df_test))\n",
    "print(\"The test RMSE is\", rmse)\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)\n",
    "# Number of zero coefficients\n",
    "zero_coeff = sum([beta==0 for beta in regression.coefficients])\n",
    "print(\"Number of coefficients equal to 0:\", zero_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical strings to index values\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "\n",
    "# One-hot encode index values\n",
    "onehot = OneHotEncoder(\n",
    "    inputCols=['org_idx', 'dow'],\n",
    "    outputCols=['org_dummy', 'dow_dummy']\n",
    ")\n",
    "\n",
    "# Assemble predictors into a single column\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy', 'dow_dummy'], outputCol='features')\n",
    "\n",
    "# A linear regression object\n",
    "regression = LinearRegression(labelCol='duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/08 07:21:25 WARN Instrumentation: [5f8a66e0] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "# Import class for creating a pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Construct a pipeline\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "\n",
    "# Split into training and testing sets in a 80:20 ratio\n",
    "df_train, df_test = df_indexed.randomSplit([0.8, 0.2], seed=43)\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "pipeline = pipeline.fit(df_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = pipeline.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Break text into tokens at non-word characters\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='terms')\n",
    "\n",
    "# Apply the hashing trick and transform to TF-IDF\n",
    "hasher = HashingTF(inputCol=\"terms\", outputCol=\"hash\")\n",
    "idf = IDF(inputCol=\"hash\", outputCol=\"features\")\n",
    "\n",
    "# Create a logistic regression object and add everything to a pipeline\n",
    "logistic = LogisticRegression()\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/08 06:43:18 WARN Instrumentation: [4f05065f] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/08 06:43:19 WARN Instrumentation: [6bdcf2e3] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/08 06:43:20 WARN Instrumentation: [e9313ea5] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/08 06:43:21 WARN Instrumentation: [7220f35a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/08 06:43:22 WARN Instrumentation: [8ace372b] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/08/08 06:43:23 WARN Instrumentation: [2a22b5d5] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create an empty parameter grid\n",
    "params = ParamGridBuilder().build()\n",
    "\n",
    "# Create objects for building and evaluating a regression model\n",
    "regression = LinearRegression(labelCol='duration')\n",
    "evaluator = RegressionEvaluator(labelCol='duration')\n",
    "\n",
    "# Create a cross validator\n",
    "cv = CrossValidator(estimator=regression, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Train and test model on multiple folds of the training data\n",
    "cv = cv.fit(df_train)\n",
    "\n",
    "# NOTE: Since cross-valdiation builds multiple models, the fit() method can take a little while to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an indexer for the org field\n",
    "indexer = StringIndexer(inputCol='org', outputCol='org_idx')\n",
    "\n",
    "# Create an one-hot encoder for the indexed org field\n",
    "onehot = OneHotEncoder(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Assemble the km and one-hot encoded fields\n",
    "assembler = VectorAssembler(inputCols=['km', 'org_dummy'], outputCol='features')\n",
    "\n",
    "# Create a pipeline and cross-validator.\n",
    "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    estimatorParamMaps=params,\n",
    "                    evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to be tested:  12\n"
     ]
    }
   ],
   "source": [
    "# Create parameter grid\n",
    "params = ParamGridBuilder()\n",
    "\n",
    "# Add grids for two parameters\n",
    "params = params.addGrid(regression.regParam, [0.01, 0.1, 1.0, 10.0]) \\\n",
    "               .addGrid(regression.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "\n",
    "# Build the parameter grid\n",
    "params = params.build()\n",
    "print('Number of models to be tested: ', len(params))\n",
    "\n",
    "# Create cross-validator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=params, evaluator=evaluator, numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|mon|dom|dow|carrier|org|depart|duration|delay|    km|label|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "|  0|  1|  2|     AA|ORD|  8.42|     155|   83|1617.0|    1|\n",
      "|  0|  1|  2|     AA|ORD| 15.25|     115|   20| 941.0|    1|\n",
      "+---+---+---+-------+---+------+--------+-----+------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Fit\n",
    "cv.fit(df_train, params=params)\n",
    "\n",
    "# Get the best model from cross validation\n",
    "best_model = cv.bestModel\n",
    "\n",
    "# Look at the stages in the best model\n",
    "print(best_model.stages)\n",
    "\n",
    "# Get the parameters for the LinearRegression object in the best model\n",
    "best_model.stages[3].extractParamMap()\n",
    "\n",
    "# Generate predictions on testing data using the best model then calculate RMSE\n",
    "predictions = best_model.transform(df_test)\n",
    "print(\"RMSE =\", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
